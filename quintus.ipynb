{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook1\n",
    "## PART 1. Document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import scipy\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "jieba.set_dictionary(\"dict.txt.big\")\n",
    "# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, paired_cosine_distances\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Adjust the number of workers if you want\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # for progress_apply\n",
    "\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_doc(\n",
    "        data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "        predictions: pd.Series,\n",
    "        mode: str = \"train\",\n",
    "        num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "            f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "            \"w\",\n",
    "            encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")  #%% md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_precision(\n",
    "        data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "        predictions: pd.Series,\n",
    ") -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "        data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "        predictions: pd.Series,\n",
    ") -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# my parameter\n",
    "least_similarity = 0.85\n",
    "save_at_least = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the stopwords\n",
    "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n",
    "from TCSP import read_stopwords_list\n",
    "\n",
    "stopwords = read_stopwords_list()\n",
    "\n",
    "\n",
    "def tokenize(text: str, stopwords: list) -> str:\n",
    "    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): claim or wikipedia article\n",
    "        stopwords (list): common words that contribute little to the meaning of a sentence\n",
    "\n",
    "    Returns:\n",
    "        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
    "    \"\"\"\n",
    "    # Windows area (because of pandarallel)\n",
    "    import jieba\n",
    "\n",
    "    tokens = list(jieba.cut(text))\n",
    "\n",
    "    return \" \".join([w for w in tokens if w not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_pred_docs_sklearn(\n",
    "        claim: str,\n",
    "        tokenizing_method: callable,\n",
    "        vectorizer: TfidfVectorizer,\n",
    "        tf_idf_matrix: scipy.sparse.csr_matrix,\n",
    "        wiki_pages: pd.DataFrame,\n",
    "        topk: int,\n",
    ") -> set:\n",
    "    tokens = tokenizing_method(claim)\n",
    "    claim_vector = vectorizer.transform([tokens])\n",
    "    # similarity_scores = tf_idf_matrix.dot(claim_vector.T).toarray()\n",
    "    similarity_scores = linear_kernel(tf_idf_matrix, claim_vector).flatten()\n",
    "\n",
    "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
    "    # similarity_scores = similarity_scores[:, 0]  # flatten the array\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    topk_sorted_indices = sorted_indices[:topk]\n",
    "\n",
    "    # Get the wiki page names based on the topk sorted indices\n",
    "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
    "\n",
    "    exact_matchs = []\n",
    "    # You can find the following code in our AICUP2023 baseline.\n",
    "    # Basically, we check if a result is exactly mentioned in the claim.\n",
    "    for result in results:\n",
    "        if (\n",
    "                (result in claim)\n",
    "                or (result in claim.replace(\" \", \"\"))  # E.g., MS DOS -> MSDOS\n",
    "                or (result.replace(\"·\", \"\") in claim)  # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
    "                or (result.replace(\"-\", \"\") in claim)  # E.g., X-SAMPA -> XSAMPA\n",
    "        ):\n",
    "            exact_matchs.append(result)\n",
    "        elif \"·\" in result:\n",
    "            splitted = result.split(\"·\")  # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
    "            for split in splitted:\n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "                elif result is results.iloc[0]:\n",
    "                    exact_matchs.append(result)\n",
    "        elif result is results.iloc[0]:\n",
    "            exact_matchs.append(result)\n",
    "\n",
    "    # exclude some documents with \"low\" similarity_score\n",
    "    if (len(exact_matchs) > save_at_least):  # at least we save n documents\n",
    "        for i in range(save_at_least, len(exact_matchs)):\n",
    "            if (paired_cosine_distances(tf_idf_matrix[topk_sorted_indices[i]], claim_vector)[0] > least_similarity):\n",
    "                exact_matchs = exact_matchs[:i]\n",
    "                break\n",
    "\n",
    "    return set(exact_matchs)\n",
    "\n",
    "# Helper function (you don't need to modify this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_title_from_evidence(evidence):\n",
    "    titles = []\n",
    "    for evidence_set in evidence:\n",
    "        if len(evidence_set) == 4 and evidence_set[2] is None:\n",
    "            return [None]\n",
    "        for evidence_sent in evidence_set:\n",
    "            titles.append(evidence_sent[2])\n",
    "    return list(set(titles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_results_to_markdown(results: dict, output_file=\"grid_search_results.md\"):\n",
    "    file_exists = Path(output_file).exists()\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        if not file_exists:\n",
    "            f.write(\"# Grid Search Results\\n\\n\")\n",
    "            f.write(\"| Experiment  | F1 Score | Precision | Recall |\\n\")\n",
    "            f.write(\"| ----------- | -------- | --------- | ------ | \\n\")\n",
    "\n",
    "        exp_name = results[\"exp_name\"]\n",
    "        f1 = results[\"f1_score\"]\n",
    "        prec = results[\"precision\"]\n",
    "        recall = results[\"recall\"]\n",
    "        f.write(f\"| {exp_name} | {f1:.4f} | {prec:.4f} | {recall:.4f} |\\n\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "wiki_path = \"data/wiki-pages\"\n",
    "min_wiki_length = 10\n",
    "num_of_samples = 500\n",
    "topk = 50\n",
    "min_df = 1\n",
    "max_df = 0.8\n",
    "use_idf = True\n",
    "sublinear_tf = True\n",
    "ngram_range = (1, 2)\n",
    "\n",
    "# Set up the experiment name for logging\n",
    "exp_name = (\n",
    "        f\"len{min_wiki_length}__top{topk}__min_df={min_df}_\"\n",
    "        + f\"max_df={max_df}__{num_of_samples}s__\" + f\"least_sim={least_similarity}__\"\n",
    "        + f\"ngram_r={ngram_range}__\" + f\"save_at_least={save_at_least}\"\n",
    ")\n",
    "if sublinear_tf:\n",
    "    exp_name = \"sublinearTF_\" + exp_name\n",
    "if not use_idf:\n",
    "    exp_name = \"no_idf_\" + exp_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First time running this cell will 34 minutes using Google Colab.\n",
    "\n",
    "wiki_cache = \"wiki\"\n",
    "target_column = \"text\"\n",
    "\n",
    "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
    "if wiki_cache_path.exists():\n",
    "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
    "else:\n",
    "    # You need to download `wiki-pages.zip` from the AICUP website\n",
    "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
    "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
    "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
    "\n",
    "    # tokenize the text and keep the result in a new column `processed_text`\n",
    "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    # save the result to a pickle file\n",
    "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)\n",
    "# Build the TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=max_df,\n",
    "    min_df=min_df,\n",
    "    use_idf=use_idf,\n",
    "    ngram_range=ngram_range,\n",
    "    sublinear_tf=sublinear_tf,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    norm=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "wiki_pages = wiki_pages[\n",
    "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
    "    ]\n",
    "corpus = wiki_pages[\"processed_text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start to encode the corpus with TF-IDF\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# fit_transform will do the following two steps:\n",
    "# 1. fit: learn the vocabulary and idf from the corpus\n",
    "# 2. transform: transform the corpus into a vector space\n",
    "# Note the result is a sparse matrix, which contains lots of zeros for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/all_test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.DataFrame(TRAIN_DATA)\n",
    "\n",
    "# Perform the prediction for document retrieval\n",
    "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        tf_idf_matrix=X,\n",
    "        wiki_pages=wiki_pages,\n",
    "        topk=topk,\n",
    "    )\n",
    ")\n",
    "precision = calculate_precision(TRAIN_DATA, train_df[\"predictions\"])\n",
    "recall = calculate_recall(TRAIN_DATA, train_df[\"predictions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.DataFrame(TEST_DATA)\n",
    "\n",
    "# Perform the prediction for document retrieval\n",
    "test_df[\"predictions\"] = test_df[\"claim\"].progress_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        tf_idf_matrix=X,\n",
    "        wiki_pages=wiki_pages,\n",
    "        topk=topk,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "if Path(doc_path).exists():\n",
    "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        predicted_results = pd.Series([\n",
    "            set(json.loads(line)[\"predicted_pages\"])\n",
    "            for line in f\n",
    "        ])\n",
    "else:\n",
    "    save_doc(TRAIN_DATA, train_df[\"predictions\"], mode=\"train\", num_pred_doc=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_path = f\"data/test_doc5.jsonl\"\n",
    "if Path(doc_path).exists():\n",
    "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        predicted_results = pd.Series([\n",
    "            set(json.loads(line)[\"predicted_pages\"])\n",
    "            for line in f\n",
    "        ])\n",
    "else:\n",
    "    save_doc(TEST_DATA, test_df[\"predictions\"], mode=\"test\", num_pred_doc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_preprocessing(mapping, df):\n",
    "    claims = []\n",
    "    pages = []\n",
    "    sentences = []\n",
    "    per_sentences = []\n",
    "    pos_sentences = []\n",
    "    labels = []\n",
    "    owned_evidence_set = set()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        searched_page = []\n",
    "\n",
    "        related_count = 0\n",
    "        # Labels which are related\n",
    "        for evidence_set in evidence_sets:\n",
    "            for evidence in evidence_set:\n",
    "                page = evidence[2]\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                page_sent_idx = str(evidence[3])\n",
    "                sentence = mapping[page][page_sent_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "\n",
    "                if (claim, page, sentence) not in owned_evidence_set:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_sent_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_sent_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_sent_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_sent_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(1)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "                    related_count += 1\n",
    "\n",
    "                    if page not in searched_page:\n",
    "                        searched_page.append(page)\n",
    "\n",
    "        not_related_count = 0\n",
    "\n",
    "        # Labels which are not related\n",
    "        for page in searched_page:\n",
    "            if not_related_count >= related_count:\n",
    "                break\n",
    "            if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                continue\n",
    "            for page_contents_idx in mapping[page]:\n",
    "                if not_related_count >= related_count:\n",
    "                    break\n",
    "\n",
    "                sentence = mapping[page][page_contents_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "\n",
    "                if (claim, page, sentence) not in owned_evidence_set:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_contents_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_contents_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_contents_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_contents_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(0)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "                    not_related_count += 1\n",
    "\n",
    "    #predicted_pages\n",
    "    for i in range(len(df)):\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        predicted_pages_sets = df[\"predicted_pages\"].iloc[i]\n",
    "\n",
    "        for page in predicted_pages_sets:\n",
    "            if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                continue\n",
    "            for page_contents_idx in mapping[page]:\n",
    "                sentence = mapping[page][page_contents_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "                pair = (claim, page, sentence)\n",
    "                if pair not in owned_evidence_set and np.random.rand(1) <= 0.1:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_contents_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_contents_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_contents_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_contents_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(0)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"claim\": claims, \"per_sentences\": per_sentences, \"sentence\": sentences, \"pos_sentences\": pos_sentences,\n",
    "         \"page\": pages, \"label\": labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_data_preprocessing(mapping, df):\n",
    "    claims = []\n",
    "    pages = []\n",
    "    sentences = []\n",
    "    per_sentences = []\n",
    "    pos_sentences = []\n",
    "    labels = []\n",
    "    owned_evidence_set = set()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        searched_page = []\n",
    "\n",
    "        # Labels which are related\n",
    "        for evidence_set in evidence_sets:\n",
    "            for evidence in evidence_set:\n",
    "                page = evidence[2]\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                page_sent_idx = str(evidence[3])\n",
    "                sentence = mapping[page][page_sent_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "\n",
    "                if (claim, page, sentence) not in owned_evidence_set:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_sent_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_sent_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_sent_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_sent_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(1)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "\n",
    "                    if page not in searched_page:\n",
    "                        searched_page.append(page)\n",
    "\n",
    "        # Labels which are not related\n",
    "        for page in searched_page:\n",
    "            if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                continue\n",
    "            for page_contents_idx in mapping[page]:\n",
    "                sentence = mapping[page][page_contents_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "\n",
    "                if (claim, page, sentence) not in owned_evidence_set:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_contents_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_contents_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_contents_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_contents_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(0)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "\n",
    "    # predicted_pages\n",
    "    for i in range(len(df)):\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        predicted_pages_sets = df[\"predicted_pages\"].iloc[i]\n",
    "\n",
    "        for page in predicted_pages_sets:\n",
    "            if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                continue\n",
    "            for page_contents_idx in mapping[page]:\n",
    "                sentence = mapping[page][page_contents_idx]\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "                pair = (claim, page, sentence)\n",
    "                if pair not in owned_evidence_set:\n",
    "                    claims.append(claim)\n",
    "                    pages.append(page)\n",
    "\n",
    "                    if str(int(page_contents_idx) - 1) in mapping[page]:\n",
    "                        per_sentences.append(mapping[page][str(int(page_contents_idx) - 1)])\n",
    "                    else:\n",
    "                        per_sentences.append(\"\")\n",
    "\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "                    if str(int(page_contents_idx) + 1) in mapping[page]:\n",
    "                        pos_sentences.append(mapping[page][str(int(page_contents_idx) + 1)])\n",
    "                    else:\n",
    "                        pos_sentences.append(\"\")\n",
    "\n",
    "                    labels.append(0)\n",
    "                    owned_evidence_set.add((claim, page, sentence))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"claim\": claims, \"per_sentences\": per_sentences, \"sentence\": sentences, \"pos_sentences\": pos_sentences,\n",
    "         \"page\": pages, \"label\": labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_version = 'hfl/chinese-lert-large'\n",
    "\n",
    "\n",
    "class BERTCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTCustom, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(model_version, return_dict=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        x = self.bert_model(input_ids, attention_mask, token_type_ids)\n",
    "        x = self.dropout1(x.pooler_output)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        claim = self.df['claim'][index]\n",
    "        per_sentence = self.df['per_sentences'][index]\n",
    "        sentence = self.df['sentence'][index]\n",
    "        pos_sentence = self.df['pos_sentences'][index]\n",
    "        page = self.df['page'][index]\n",
    "        label = self.df['label'][index]\n",
    "        label = int(label)\n",
    "\n",
    "        full_input = claim + \"[SEP]\" + page + \"[SEP]\" + per_sentence + \"[SEP]\" + sentence + \"[SEP]\" + pos_sentence\n",
    "\n",
    "        input = self.tokenizer.encode_plus(\n",
    "            full_input,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        out = {\n",
    "            \"input_ids\": input[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": input[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": input[\"token_type_ids\"].flatten(),\n",
    "            \"targets\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/all_test.jsonl\")\n",
    "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n",
    "TRAIN_GT, DEV_GT = train_test_split(\n",
    "    DOC_DATA,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\n",
    "        f\"data/Part2\"):\n",
    "    os.makedirs(\n",
    "        f\"data/Part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(TRAIN_GT).to_json(\"data/Part2/train_doc5.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "pd.DataFrame(DEV_GT).to_json(\"data/Part2/dev_doc5.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_preprocessed = train_data_preprocessing(mapping, pd.DataFrame(TRAIN_GT))\n",
    "\n",
    "total_training_samples = len(train_preprocessed)\n",
    "print(f\"train_preprocessed length: {total_training_samples}\")\n",
    "class_0_samples = train_preprocessed[\"label\"].value_counts()[0]\n",
    "class_1_samples = train_preprocessed[\"label\"].value_counts()[1]\n",
    "\n",
    "weight_for_class_0 = total_training_samples / class_0_samples\n",
    "weight_for_class_1 = total_training_samples / class_1_samples\n",
    "\n",
    "print(weight_for_class_0)\n",
    "print(weight_for_class_1)\n",
    "\n",
    "print(train_preprocessed[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_val_preprocessed = eval_data_preprocessing(mapping, pd.DataFrame(DEV_GT))\n",
    "\n",
    "print(f\"eval_val_preprocessed length: {len(eval_val_preprocessed)}\")\n",
    "print(eval_val_preprocessed[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claim = train_preprocessed['claim'][100]\n",
    "sentence = train_preprocessed['sentence'][100]\n",
    "per_sentence = train_preprocessed['per_sentences'][100]\n",
    "pos_sentence = train_preprocessed['pos_sentences'][100]\n",
    "page = train_preprocessed['page'][100]\n",
    "label = train_preprocessed['label'][100]\n",
    "label = int(label)\n",
    "\n",
    "print(claim)\n",
    "print(page)\n",
    "print(sentence)\n",
    "print(label)\n",
    "print(torch.tensor(label, dtype=torch.float))\n",
    "\n",
    "full_input = claim + \"[SEP]\" + page + \"[SEP]\" + sentence\n",
    "\n",
    "input = tokenizer.encode_plus(\n",
    "    full_input,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = WikiDataset(train_preprocessed, tokenizer)\n",
    "full_eval_dataset = WikiDataset(eval_val_preprocessed, tokenizer)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "eval_val_dataloader = torch.utils.data.DataLoader(\n",
    "    full_eval_dataset,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = BERTCustom()\n",
    "model.to(device)\n",
    "\n",
    "pos_weight = torch.tensor([weight_for_class_1]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(n_epochs, training_loader, eval_val_dataloader, model, optimizer, loss_fn, scheduler):\n",
    "    f1_record = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = []\n",
    "        eval_loss = []\n",
    "\n",
    "        all_eval_preds = []\n",
    "        all_eval_targets = []\n",
    "\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        training_progress_bar = tqdm(enumerate(training_loader), total=len(training_loader))\n",
    "        training_progress_bar.set_description(f'Epoch Training {epoch}')\n",
    "        for batch_index, batch in enumerate(training_loader):\n",
    "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long).squeeze()\n",
    "            targets = targets.view(-1, 1).float()\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Convert the logits to probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Convert the probabilities to binary predictions\n",
    "            preds = (probs > 0.7).float()\n",
    "\n",
    "            # Now flatten the tensors and convert them to numpy arrays for use with sklearn\n",
    "            preds = preds.view(-1).cpu().numpy()\n",
    "            targets = targets.view(-1).cpu().numpy()\n",
    "\n",
    "            all_train_preds.extend(preds)\n",
    "            all_train_targets.extend(targets)\n",
    "            training_progress_bar.update()\n",
    "\n",
    "        training_progress_bar.close()\n",
    "\n",
    "        report = classification_report(all_train_targets, all_train_preds, zero_division=0)\n",
    "        print(report)\n",
    "\n",
    "        precision = precision_score(all_train_targets, all_train_preds, zero_division=0)\n",
    "        recall = recall_score(all_train_targets, all_train_preds, zero_division=0)\n",
    "        f1 = f1_score(all_train_targets, all_train_preds, zero_division=0)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} Training loss {sum(train_loss) / len(train_loss):.4f} precision {precision:.4f} recall {recall:.4f} f1 {f1:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        evaling_progress_bar = tqdm(enumerate(eval_val_dataloader), total=len(eval_val_dataloader))\n",
    "        evaling_progress_bar.set_description(f'Epoch Evaling {epoch}')\n",
    "        with torch.no_grad():\n",
    "            for batch_index, batch in enumerate(eval_val_dataloader):\n",
    "                input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "                attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "                token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "                targets = batch['targets'].to(device, dtype=torch.long).squeeze()\n",
    "                targets = targets.view(-1, 1).float()\n",
    "                outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                eval_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.7).float()\n",
    "                preds = preds.view(-1).cpu().numpy()\n",
    "                targets = targets.view(-1).cpu().numpy()\n",
    "\n",
    "                all_eval_preds.extend(preds)\n",
    "                all_eval_targets.extend(targets)\n",
    "\n",
    "                evaling_progress_bar.update()\n",
    "\n",
    "        evaling_progress_bar.close()\n",
    "\n",
    "        report = classification_report(all_eval_targets, all_eval_preds, zero_division=0)\n",
    "        print(report)\n",
    "\n",
    "        precision = precision_score(all_eval_targets, all_eval_preds, zero_division=0)\n",
    "        recall = recall_score(all_eval_targets, all_eval_preds, zero_division=0)\n",
    "        f1 = f1_score(all_eval_targets, all_eval_preds, zero_division=0)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} Evaling loss {sum(eval_loss) / len(eval_loss):.4f} precision {precision:.4f} recall {recall:.4f} f1 {f1:.4f}\")\n",
    "\n",
    "        if f1 > f1_record:\n",
    "            print(f\"Record: {f1_record}\")\n",
    "            f1_record = f1\n",
    "            print(f\"Best f1 now: {f1_record}\")\n",
    "            if not os.path.exists(\n",
    "                    f\"checkpoints/sent_retrieval/{model_version.replace('/', '_')}\"):\n",
    "                os.makedirs(\n",
    "                    f\"checkpoints/sent_retrieval/{model_version.replace('/', '_')}\")\n",
    "            torch.save(model.state_dict(), f\"checkpoints/sent_retrieval/{model_version.replace('/', '_')}/val_best.pt\")\n",
    "\n",
    "        scheduler.step(sum(eval_loss) / len(eval_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained = train_model(10, train_dataloader, eval_val_dataloader, model, optimizer, loss_fn, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def largest_five_values(lst):\n",
    "    # Enumerate the list, providing (index, value) tuples\n",
    "    enumerated_lst = list(enumerate(lst))\n",
    "\n",
    "    # Sort the enumerated list by the value in each tuple (i.e., the second element in the tuple)\n",
    "    # Set reverse=True to sort in descending order\n",
    "    sorted_lst = sorted(enumerated_lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the first five elements from the sorted list\n",
    "    return sorted_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_all(json_file, model_file, mapping, tokenizer, device):\n",
    "    data = load_json(json_file)\n",
    "    data = pd.DataFrame(data)\n",
    "    data_length = len(data)\n",
    "    predicted_evidences = []\n",
    "\n",
    "    part2_model = BERTCustom()\n",
    "    part2_model.to(device)\n",
    "    part2_model.load_state_dict(\n",
    "        torch.load(model_file))\n",
    "    part2_model.eval()\n",
    "\n",
    "    progress_bar = tqdm(total=data_length, desc='Predicting')\n",
    "    for i in range(data_length):\n",
    "        predicted_evidence = []\n",
    "        holding_evidence = []\n",
    "        holding_probs = []\n",
    "\n",
    "        predicted_pages = data[\"predicted_pages\"][i]\n",
    "        claim = data[\"claim\"][i]\n",
    "\n",
    "        for page in predicted_pages:\n",
    "            for page_idx in mapping[page]:\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    if str(int(page_idx) - 1) in mapping[page]:\n",
    "                        per_sentence = mapping[page][str(int(page_idx) - 1)]\n",
    "                    else:\n",
    "                        per_sentence = \"\"\n",
    "\n",
    "                    sentence = mapping[page][page_idx]\n",
    "\n",
    "                    if str(int(page_idx) + 1) in mapping[page]:\n",
    "                        pos_sentence = mapping[page][str(int(page_idx) + 1)]\n",
    "                    else:\n",
    "                        pos_sentence = \"\"\n",
    "\n",
    "                    if sentence == \"\":\n",
    "                        continue\n",
    "\n",
    "                    full_input = claim + \"[SEP]\" + page + \"[SEP]\" + per_sentence + \"[SEP]\" + sentence + \"[SEP]\" + pos_sentence\n",
    "\n",
    "                    input = tokenizer.encode_plus(\n",
    "                        full_input,\n",
    "                        None,\n",
    "                        add_special_tokens=True,\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=256,\n",
    "                        truncation=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "\n",
    "                    input_ids = input['input_ids'].to(device, dtype=torch.long)\n",
    "                    attention_mask = input['attention_mask'].to(device, dtype=torch.long)\n",
    "                    token_type_ids = input['token_type_ids'].to(device, dtype=torch.long)\n",
    "                    outputs = part2_model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    preds = (probs > 0.7).float()\n",
    "                    preds = preds.view(-1).cpu().numpy()\n",
    "\n",
    "                    holding_evidence.append([page, int(page_idx)])\n",
    "                    holding_probs.append(probs.detach().cpu().item())\n",
    "\n",
    "        top_5 = largest_five_values(holding_probs)\n",
    "\n",
    "        for tup in top_5:\n",
    "            predicted_evidence.append(holding_evidence[tup[0]])\n",
    "\n",
    "        if len(predicted_evidence) < 1:\n",
    "            holding_probs = np.array(holding_probs)\n",
    "            max_index = np.argmax(holding_probs)\n",
    "            predicted_evidence.append(holding_evidence[max_index])\n",
    "\n",
    "        predicted_evidences.append(predicted_evidence)\n",
    "\n",
    "        progress_bar.update()\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    data[\"predicted_evidence\"] = predicted_evidences\n",
    "\n",
    "    file_name = json_file.split(\".\")\n",
    "    file_name[0] += \"sent5\"\n",
    "    file_name = \".\".join(file_name)\n",
    "\n",
    "    data.to_json(file_name, orient='records', lines=True, force_ascii=False)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_all(\"data/Part2/train_doc5.jsonl\", \"checkpoints/sent_retrieval/hfl_chinese-lert-large/val_best.pt\", mapping,\n",
    "            tokenizer,\n",
    "            device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_all(\"data/Part2/dev_doc5.jsonl\", \"checkpoints/sent_retrieval/hfl_chinese-lert-large/val_best.pt\", mapping,\n",
    "            tokenizer,\n",
    "            device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_all(\"data/test_doc5.jsonl\", \"checkpoints/sent_retrieval/hfl_chinese-lert-large/val_best.pt\", mapping,\n",
    "            tokenizer,\n",
    "            device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7"
   },
   "source": [
    "notebook3\n",
    "## PART 3. Claim verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgA1vcUyzjlx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/Part2/train_doc5sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/Part2/dev_doc5sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/Part2/train_doc5sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/Part2/dev_doc5sent5.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (same as part 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICUP dataset with top-k evidence sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        evidence = item[\"evidence_list\"]\n",
    "\n",
    "        # In case there are less than topk evidence sentences\n",
    "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
    "        evidence += pad\n",
    "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
    "\n",
    "        concat = self.tokenizer(\n",
    "            concat_claim_evidence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "            \n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(**batch).logits\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.extend(pred.tolist())\n",
    "    return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    # if mode == \"eval\":\n",
    "    # extract evidence\n",
    "    df[\"evidence_list\"] = df[\"predicted_evidence\"].map(lambda x: [\n",
    "        mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "        for evi_id, evi_idx in x  # for each evidence list\n",
    "    ][:topk] if isinstance(x, list) else [])\n",
    "   \n",
    "    \n",
    "    # # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    # if \"evidence\" in df.columns:\n",
    "    #     df[\"evidence\"] = df[\"evidence\"].map(\n",
    "    #         lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "    #         if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    # print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    # if mode == \"eval\":\n",
    "    # # extract evidence\n",
    "    #     df[\"evidence_list\"] = df[\"predicted_evidence\"].map(lambda x: [\n",
    "    #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "    #         for evi_id, evi_idx in x  # for each evidence list\n",
    "    #     ][:topk] if isinstance(x, list) else [])\n",
    "    # else:\n",
    "    #     # extract evidence\n",
    "    #     df[\"evidence_list\"] = df[\"evidence\"].map(lambda x: [\n",
    "    #         \" \".join([  # join evidence\n",
    "    #             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "    #             for _, _, evi_id, evi_idx in evi_list\n",
    "    #         ]) if isinstance(evi_list, list) else \"\"\n",
    "    #         for evi_list in x  # for each evidence list\n",
    "    #     ][:1] if isinstance(x, list) else [])\n",
    "\n",
    "    #     df2 = df.copy()\n",
    "    #     df2[\"evidence_list\"] = df2[\"predicted_evidence\"].map(lambda x: [\n",
    "    #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "    #         for evi_id, evi_idx in x  # for each evidence list\n",
    "    #     ][:topk] if isinstance(x, list) else [])\n",
    "\n",
    "    #     for i in range(0, len(df)):\n",
    "    #         if len(df[\"evidence_list\"][i][0]) == 0:\n",
    "    #             df[\"evidence_list\"][i][0] = df2[\"evidence_list\"][i][0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "# MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "# MODEL_NAME = \"hfl/chinese-xlnet-mid\"\n",
    "# MODEL_NAME = \"hfl/chinese-lert-base\"\n",
    "# MODEL_NAME = \"hfl/chinese-lert-large\"\n",
    "# MODEL_NAME = \"hfl/chinese-pert-base\"\n",
    "# MODEL_NAME = \"hfl/chinese-macbert-base\"\n",
    "# MODEL_NAME = \"uer/sbert-base-chinese-nli\"\n",
    "# MODEL_NAME = \"bert-base-chinese\"\n",
    "\n",
    "MODEL_NAME = \"hfl/chinese-lert-large\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "SEED = 42  #@param {type:\"integer\"}\n",
    "LR = 2e-5  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 20  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
    "EVIDENCE_TOPK = 3  #@param {type:\"integer\"}\n",
    "VALIDATION_STEP = 50  #@param {type:\"integer\"}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "\n",
    "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_PKL_FILE.exists():\n",
    "    train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
    "        train_df = pickle.load(f)\n",
    "\n",
    "if not DEV_PKL_FILE.exists():\n",
    "    dev_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(DEV_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "        dev_df = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0rVk3990DlD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "val_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_count = pd.DataFrame(TRAIN_DATA)\n",
    "\n",
    "train_df_count_samples = len(train_df_count)\n",
    "print(f\"train_preprocessed length: {train_df_count_samples}\")\n",
    "\n",
    "class_0 = train_df_count[\"label\"].value_counts()[\"supports\"]\n",
    "class_1 = train_df_count[\"label\"].value_counts()[\"refutes\"]\n",
    "class_2 = train_df_count[\"label\"].value_counts()[\"NOT ENOUGH INFO\"]\n",
    "\n",
    "total_samples = class_0 + class_1 + class_2\n",
    "\n",
    "frequency_0 = class_0 / total_samples\n",
    "frequency_1 = class_1 / total_samples\n",
    "frequency_2 = class_2 / total_samples\n",
    "\n",
    "weight_0 = 1 / frequency_0\n",
    "weight_1 = 1 / frequency_1\n",
    "weight_2 = 1 / frequency_2\n",
    "\n",
    "total_weight = weight_0 + weight_1 + weight_2\n",
    "\n",
    "weight_0 /= total_weight\n",
    "weight_1 /= total_weight\n",
    "weight_2 /= total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzMgs-Zs3sTN"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL2ID),\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "class_weights = torch.tensor([weight_0, weight_1, weight_2]).float().to(device)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aqMjEek3wmu"
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "average_acc = 0.0\n",
    "number = 0\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    print(epoch)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        # loss = outputs.loss\n",
    "        # loss.backward()\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fct(logits.view(-1, logits.shape[-1]), batch['labels'].view(-1))\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            # print(\"Start validation\")\n",
    "            number += 1\n",
    "\n",
    "            val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "                if metric_name == \"val_acc\":\n",
    "                    average_acc += metric_value\n",
    "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                CKPT_DIR,\n",
    "                current_steps,\n",
    "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLkfuoAE49mz"
   },
   "outputs": [],
   "source": [
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "test_df = join_with_topk_evidence(\n",
    "    pd.DataFrame(TEST_DATA),\n",
    "    mapping,\n",
    "    mode=\"eval\",\n",
    "    topk=EVIDENCE_TOPK,\n",
    ")\n",
    "test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
    "\n",
    "test_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqIjlht8yCMA"
   },
   "outputs": [],
   "source": [
    "# CKPT_DIR = \"checkpoints/\" + f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_{EVIDENCE_TOPK}\"\n",
    "CKPT_DIR = \"checkpoints/claim_verification/\"\n",
    "ckpt_name = \"val_acc=0.7350_model.3050\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "predicted_label = run_predict(model, test_dataloader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gl9I3ZWW4pHo"
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df.copy()\n",
    "\n",
    "for i in range(0, len(predict_dataset)):\n",
    "    if len(predict_dataset[\"predicted_evidence\"][i]) > 5:\n",
    "        predict_dataset[\"predicted_evidence\"][i] = predict_dataset[\"predicted_evidence\"][i][:5] \n",
    "\n",
    "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀取 JSON 檔案\n",
    "data = load_json(\"submission.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 依照 id 欄位排序\n",
    "df = df.sort_values(\"id\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "df.to_json(\n",
    "    \"OUTPUT_NAME\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
